{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26db76b4-0c81-4ca7-bc91-eef5b8a4bcf1",
   "metadata": {},
   "source": [
    "# Unit testing in R with `testthat`\n",
    "\n",
    "## Course objectives\n",
    "\n",
    "- Understand why automated testing is valuable.\n",
    "- Learn how to use the `testthat` package to write tests for functions in R.\n",
    "- Learn a standard practice for organising your functions and tests.\n",
    "- Gain some tips and advice for writing tests.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Most people test their code in a manual, ad-hoc fashion. In R, this might typically be in the form of sourcing particular lines in a script, or trying out a function in the console with one or two different inputs and checking by eye that it gives the right answer.\n",
    "\n",
    "While this is fine up to a point, there is a lot of value in turning testing into an automated process, that is, writing code that runs tests for us. There are several reasons why automated tests are valuable:\n",
    "- **It leads to fewer bugs.** When we test we often take a more critical view of our code and start to consider all the 'unexpected' cases where it could go wrong. This helps us identify potential bugs before they become a problem, making our code more robust.\n",
    "- **They provide a safety net for us when making changes to code.** Have you ever encountered a situation when you changed some bit of code, but then this lead to an error elsewhere in the code which you didn't foresee? Having a suite of automated tests that we can run whenever we make changes to code gives us confidence that our change didn't unexpectely cause incorrect behaviour in another part of the code. This is especially valuable when we're coming back to code after time away and have forgotten the details of how it works, or if we're working on someone else's code: having a suite of tests gives us much more confidence to make changes without fear of unexpected bugs creeping in.\n",
    "- **It nearly always improves the design of our code.** Writing tests for our code encourages us to structure it in such a way that it is easy to test, which often means breaking it down into simpler pieces (functions) that pass data between each other: this is often easier to understand and modify. Furthermore, writing tests forces us to really think about _what exactly the code needs to do_, in a range of scenarios. The act of writing tests is a great tool for bringing clarity to our thinking.\n",
    "- **It gives others confidence in our code.** Having a suite of tests demonstrates that we've taken the time to check it works correctly and provides a precise record of what has been tested (and what might not have been tested).\n",
    "\n",
    "This tutorial will guide you through writing unit tests in R, using the [`testthat` package](https://testthat.r-lib.org/). Unit testing, for the purposes of this tutorial, means testing functions that we've written ourselves, to check that they behave correctly on different inputs. This tutorial therefore assumes that you have some experience with writing functions in R and using these in your code.\n",
    "\n",
    "You can install `testthat` from CRAN in the usual way, using `install.package` or `renv`:\n",
    "\n",
    "```r\n",
    "install.packages(\"testthat\")\n",
    "\n",
    "# Or if using renv for package management:\n",
    "renv::install(\"testthat\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### A note on `testthat` before we begin\n",
    "\n",
    "The `testthat` package is designed mainly for writing tests for functions that are in an R package. The [R Packages book by Hadley Wickham and Jennifer Bryan](https://r-pkgs.org/) (2nd ed at time of writing) provides an excellent handbook for creating your own R packages, including the use of `testthat` for writing tests for the package. This tutorial presents a simplified and somewhat unorthodox use of `testthat` that doesn't require your code to be in a package. However, in the long term you may want to consider putting your functions into a package to use `testthat` 'properly' (besides this, it's a great way of sharing your functions with others)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9cf4b37-d013-414d-a122-bfb52fda99e5",
   "metadata": {},
   "source": [
    "## Setting up your test files\n",
    "\n",
    "Generally we keep our tests in separate files to our main R source files. The recommended practice is to have the following structure in our project:\n",
    "\n",
    "```\n",
    "MyAnalysis/\n",
    "â”œâ”€â”€ R/\n",
    "â”‚Â Â  â”œâ”€â”€ data_cleaning.R\n",
    "â”‚Â Â  â”œâ”€â”€ data_loading.R\n",
    "â”‚Â Â  â””â”€â”€ utilities.R\n",
    "â”œâ”€â”€ ... <other files and folders>\n",
    "â””â”€â”€ tests/\n",
    "    â””â”€â”€ testthat/\n",
    "        â”œâ”€â”€ test_data_cleaning.R\n",
    "        â”œâ”€â”€ test_data_loading.R\n",
    "        â””â”€â”€ test_utilities.R\n",
    "```\n",
    "\n",
    "Observe the following:\n",
    "\n",
    "- All files of functions are kept in a folder called `R`. This is a standard pattern with R codebases, particularly with R packages. We suggest you keep to this pattern.\n",
    "- All files containing tests **must** begin with either `test_` or `test-`, because `testthat` assumes this naming convention when looking for tests to run.\n",
    "- It is very common practice to have one test file per file of functions, with the test file name derived from the file it corresponds to. This keeps the tests organised and easy to navigate. Only deviate from this convention if you have a compelling reason to.\n",
    "- It's not strictly necessary to keep the tests in a subfolder `testthat` of `tests`. But we recommend this because (1) it's the layout that `testthat` is designed to work with; (2) it is the layout required by `testthat` if you later decide to turn your project into an R package; and (3) it is the layout other developers familiar testing using `testthat` will expect, so is easier for them to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b629de3-50a6-4824-ba74-ab5848b57692",
   "metadata": {},
   "source": [
    "## Writing and running your first test\n",
    "\n",
    "For the purposes of this tutorial, we'll assume that our functions to be tested live in the file `R/functions.R` and that we write the corresponding tests in `tests/testthat/test_functions.R`.\n",
    "\n",
    "Suppose we want to test the following function (the classic ['fizzbuzz' function](https://en.wikipedia.org/wiki/Fizz_buzz)). The function takes in a number and does one of the following:\n",
    "- returns the string \"fizz\" if the number is divisible by 3;\n",
    "- returns the string \"buzz\" if it's divisible by 5;\n",
    "- returns the string \"fizzbuzz\" if it's divisible by both 3 and 5; and\n",
    "- returns the number itself if it's not divisible by 3 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36e3fc1-d817-4904-a3bd-18513d6e56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fizzbuzz <- function(n) {\n",
    "  if (n %% 15 == 0) {\n",
    "    return(\"fizzbuzz\")\n",
    "  } else if (n %% 3 == 0) {\n",
    "    return(\"fizz\")\n",
    "  } else if (n %% 5 == 0) {\n",
    "    return(\"buzz\")\n",
    "  } else {\n",
    "    return(n)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510d775-3450-4a05-92f2-671bb0296506",
   "metadata": {},
   "source": [
    "Our first test checks that `fizzbuzz` of 1 is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11073a16-33be-4d91-9055-0c156d496fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTest passed\u001b[39m ğŸ¥³\n"
     ]
    }
   ],
   "source": [
    "library(testthat)\n",
    "\n",
    "test_that(\"fizzbuzz of 1 is 1\", {\n",
    "  expect_equal(fizzbuzz(1), 1)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dcf403-a586-4f84-a2f6-f116b71cef59",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "\n",
    "- The test is contained within the `test_that` function. The first argument to `test_that` is a string giving a description of what's being tested (in this case, `\"fizzbuzz of 1 is 1\"`). The second argument is a block of code that will be executed for the test, which has to be contained in curly braces.\n",
    "- The block of test code is simple in this case. It contains the function `expect_equal` that is used to check whether the first argument (`fizzbuzz(1)` in this case) is equal to the second argument (`1` in this case). This function is an example of what `testthat` refers to as _expectations_: these are just functions that are used by `testthat` to check whether some condition is `TRUE`/`FALSE` or that some other thing happened (such as an error being raised, for example). We'll see a few more expectations later.\n",
    "\n",
    "> **Note: We need to use expectations**\n",
    ">\n",
    "> You might wonder whether we could have instead not used `expect_equal` and instead just written\n",
    "> ```r\n",
    "> test_that(\"fizzbuzz of 1 is 1\", {\n",
    ">   fizzbuzz(1) == 1\n",
    "> })\n",
    "> ```\n",
    "> The answer is no. Expectations like `expect_that` are part of the general machinery that `testthat` uses when running several tests at once, so we need to use these instead of our code to perform checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb2719c-79de-4cea-98e1-bf9a70c85300",
   "metadata": {},
   "source": [
    "### Putting into a file\n",
    "\n",
    "Here is how we would put this test into a test file:\n",
    "\n",
    "```r\n",
    "# Source the file containing our functions to be tested\n",
    "source(\"../../R/functions.R\")\n",
    "\n",
    "test_that(\"fizzbuzz of 1 is 1\", {\n",
    "  expect_equal(fizzbuzz(1), 1)\n",
    "})\n",
    "```\n",
    "\n",
    "**Note:** we need to source the file `R/functions.R` with a path relative to the directory where the test file `test_functions.R` lives!\n",
    "\n",
    "We can then run the tests in this file by running the following in an R console (assuming our working directory is the root folder of our project):\n",
    "\n",
    "```r\n",
    "# From the root folder of our project\n",
    "testthat::test_dir(\"tests/testthat\")\n",
    "\n",
    "```\n",
    "\n",
    "You should see output similar to the following:\n",
    "\n",
    "```r\n",
    "> testthat::test_dir(\"tests/testthat\")\n",
    "âœ” | F W  S  OK | Context\n",
    "âœ” |          1 | functions                                \n",
    "\n",
    "â•â• Results â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]\n",
    "\n",
    "Way to go!\n",
    "```\n",
    "\n",
    "This table shows us that our single test passed!\n",
    "\n",
    "What if our test had failed? Let's temporarily change our function `fizzbuzz` to incorrectly return `2` instead of `1`. In that case, we would see something like the following:\n",
    "\n",
    "```r\n",
    "> testthat::test_dir(\"tests/testthat\")\n",
    "âœ” | F W  S  OK | Context\n",
    "âœ– | 1        0 | functions                                \n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Failure (test_functions.R:4:3): fizzbuzz of 1 is 1\n",
    "fizzbuzz(1) not equal to 1.\n",
    "1/1 mismatches\n",
    "[1] 2 - 1 == 1\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "â•â• Results â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "â”€â”€ Failed tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Failure (test_functions.R:4:3): fizzbuzz of 1 is 1\n",
    "fizzbuzz(1) not equal to 1.\n",
    "1/1 mismatches\n",
    "[1] 2 - 1 == 1\n",
    "\n",
    "[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]\n",
    "Error: Test failures\n",
    "\n",
    "```\n",
    "\n",
    "Notice how the summary table shows there was a single test failure (and no passes), and that a description of which test failed, as well as how it failed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7234fa-0782-4816-9313-e7cf67622376",
   "metadata": {},
   "source": [
    "#### Exercise: Running your first test\n",
    "\n",
    "- If you haven't done so already, put the `fizzbuzz` function into an R source file and put the tests into a corresponding test file, and run the tests from the R console using `testthat::test_dir`. We suggest following the template project structure above.\n",
    "- Once you have done this, try modifying the test and/or function to get some test failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5841a-921d-48ea-9cc5-efb9033dc5d5",
   "metadata": {},
   "source": [
    "## Adding more tests\n",
    "\n",
    "Adding more tests is simply a matter of adding more `test_that` calls within the test file:\n",
    "\n",
    "```r\n",
    "# In tests/testthat/test_functions.R\n",
    "\n",
    "source(\"../../R/functions.R\")\n",
    "\n",
    "test_that(\"fizzbuzz of 1 is 1\", {\n",
    "  expect_equal(fizzbuzz(1), 1)\n",
    "})\n",
    "\n",
    "test_that(\"fizzbuzz of 2 is 2\", {\n",
    "  expect_equal(fizzbuzz(2), 2)\n",
    "})\n",
    "\n",
    "test_that(\"fizzbuzz of 3 is 'fizz'\", {\n",
    "  expect_equal(fizzbuzz(3), \"fizz\")\n",
    "})\n",
    "\n",
    "```\n",
    "\n",
    "When we run `testthat::test_dir(\"tests/testthat\")`, each of these tests will run and a summary of the results printed. As above, if there are any test failures or errors, these will be reported individually.\n",
    "\n",
    "```r\n",
    "> testthat::test_dir(\"tests/testthat\")\n",
    "âœ” | F W  S  OK | Context\n",
    "âœ” |          3 | functions                                \n",
    "\n",
    "â•â• Results â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f633f-47a7-4270-bbd6-d6e23b3e9ebd",
   "metadata": {},
   "source": [
    "### Exercise: add your own tests\n",
    "\n",
    "- Add more tests to the test file, checking that the correct output is given for the inputs `n = 5` and `n = 15`.\n",
    "- What other values do you think should be checked? Add tests for these cases too. (Hint: notice the `%%` in the `if` conditions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a29fff-206d-494b-ab86-290dc9947260",
   "metadata": {},
   "source": [
    "## Examples of other expectations\n",
    "\n",
    "So far we have only use the `expect_equal` expectation in our tests, however there are others we can use.\n",
    "\n",
    "### Testing for errors with `expect_error`\n",
    "\n",
    "Going back to our `fizzbuzz` example, notice that there's been a bit of an implict assumption that the input is an integer. But the code will actually work fine even if we supply a non-integer (try it!). Perhaps we really want to enforce that only integer arguments `n` are allowed and that if `n` isn't an integer, then an error will be raised instead. Raising errors is a good way to flag to us / other people when some bad or unwanted input data was received by a function, stopping the code from doing any further computations with this bad data.\n",
    "\n",
    "How could we test this? The `testthat` package provides the `expect_error` expectation for this. The basic form for using this is:\n",
    "\n",
    "```r\n",
    "expect_error(expr, msg_regexp)\n",
    "```\n",
    "where:\n",
    "- `expr` is the expression to test (e.g. `fizzbuzz(3.5)`)\n",
    "- `msg_regexp` is a character vector giving a regular expression that the error message should match, or `NULL` (the default) to not check the message. Often it's simplest just to write out the whole message, in which case you can also specify the optional argument `fixed = TRUE` to ensure the error message matches `msg_regexp` exactly.\n",
    "\n",
    "Here's an example of using `expect_error` to check that `fizzbuzz` gives an error with a specific message, if a non-integer number is provided:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a314bd11-a863-4d0c-ad1a-4a597e1dcdb3",
   "metadata": {},
   "source": [
    "```r\n",
    "test_that(\"fizzbuzz of non-integers gives error\", {\n",
    "  expect_error(\n",
    "    fizzbuzz(3.5),\n",
    "    \"expected `n` to be an integer, but received a non-integer number instead\",\n",
    "    fixed = TRUE  # to check the error message is equal to the above message\n",
    "  )\n",
    "})\n",
    "\n",
    "```\n",
    "\n",
    "```r\n",
    "â”€â”€ Failure: fizzbuzz of non-integers gives error â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "`fizzbuzz(3.5)` did not throw an error.\n",
    "\n",
    "Error:\n",
    "! Test failed\n",
    "Traceback:\n",
    "\n",
    "1. (function () \n",
    " . expr)()\n",
    "2. reporter$stop_if_needed()\n",
    "3. abort(\"Test failed\", call = NULL)\n",
    "4. signal_abort(cnd, .file)\n",
    "5. signalCondition(cnd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76ba88-f58a-4910-91cf-3699753272e4",
   "metadata": {},
   "source": [
    "The test failure is expected here because we haven't yet updated our `fizzbuzz` function to raise the error! Let's rectify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c46be76-1d31-4964-ad48-8b8005383fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fizzbuzz <- function(n) {\n",
    "  # Check that input is an integer\n",
    "  if (trunc(n) != n) {\n",
    "    stop(\"expected `n` to be an integer, but received a non-integer number instead\")\n",
    "  }\n",
    "  \n",
    "  if (n %% 15 == 0) {\n",
    "    return(\"fizzbuzz\")\n",
    "  } else if (n %% 3 == 0) {\n",
    "    return(\"fizz\")\n",
    "  } else if (n %% 5 == 0) {\n",
    "    return(\"buzz\")\n",
    "  } else {\n",
    "    return(n)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c309a67-3ca8-4907-8f6f-c2372588db72",
   "metadata": {},
   "source": [
    "Now running our new test will pass, because the error is raised with the correct message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c40b15-18db-4476-8fdb-9508b5e632af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTest passed\u001b[39m ğŸ¥‡\n"
     ]
    }
   ],
   "source": [
    "test_that(\"fizzbuzz of non-integers gives error\", {\n",
    "  expect_error(\n",
    "    fizzbuzz(3.5),\n",
    "    \"expected `n` to be an integer, but received a non-integer number instead\",\n",
    "    fixed = TRUE  # to check the error message is equal to the above message\n",
    "  )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c4472-8b09-489d-9170-6d200de9da6d",
   "metadata": {},
   "source": [
    "#### Exercise: Testing negative `n`\n",
    "\n",
    "- If you haven't done so already, update your version of `fizzbuzz` to include the error check and your test file to include the test for the error.\n",
    "- The function `fizzbuzz` currently works if the argument `n` is any integer, positive or negative. Modify `fizzbuzz` to handle the case where `n` is negative separately and write a test to verify it works. You can do this however you like, but here are three possibilities:\n",
    "  * Raise an error (like we did above).\n",
    "  * Return the value as currently calculated but also issue the user with a warning. See the base R function `warning` and use the [`expect_warning` expectation](https://testthat.r-lib.org/reference/expect_error.html) in your test.\n",
    "  * Return the value as currently calculated but also print a message to the console using `cat`. Use the [`expect_output` expectation](https://testthat.r-lib.org/reference/expect_output.html) to test this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2c626-f967-4a36-bc27-553253e3292e",
   "metadata": {},
   "source": [
    "### Other common expectations\n",
    "\n",
    "The [`testthat` documentation lists out all the different expectations](https://testthat.r-lib.org/reference/index.html#expectations) it offers for writing tests. Here are a selection we think are particularly useful:\n",
    "- `expect_equal` (obviously)\n",
    "- `expect_null`: does code return `NULL`?\n",
    "- `expect_true`, `expect_false`: does code return `TRUE` or `FALSE`?\n",
    "- `expect_match`, `expect_no_match`: does a string match a regular expression (or not)?\n",
    "- `expect_length`: does code return a vector of a given length?\n",
    "- `expect_named`: does code return a vector with given names? (Can be used to check that a dataframe has given column names.)\n",
    "- `expect_setequal(x, y)`: do `x` and `y` define the same sets i.e. have the same elements up to repetition and change of order.\n",
    "- `expect_in(x, y)`: is every element in `x` also in `y`?\n",
    "- `expect_mapequal(x, y)`: do `x` and `y` have the same names and `x[names(y)] == y`?\n",
    "- `expect_error`, `expect_warning`, `expect_output`: does code raise an error, issue a warning, or print output to the console?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd029b76-4935-419f-a77d-472038c38b70",
   "metadata": {},
   "source": [
    "#### Exercise: Other expectations\n",
    "\n",
    "Write your own function and test that uses an expectation that you haven't used yet. The function doesn't have to do anything sensible; the point is to gain practice using different expectations and reading the `testthat` documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729468c-1e33-44e2-9181-f03a3e69417c",
   "metadata": {},
   "source": [
    "## A more complicated example\n",
    "\n",
    "The tests we've seen so far have been very simple: just calling one line of code and checking for a particular output / error etc. Often though you will write functions that may need more code to write a good test. In this section we'll look at a more realistic example and leave our toy `fizzbuzz` example behind.\n",
    "\n",
    "> **Use of `dplyr`**\n",
    ">\n",
    "> This part of the notes assumes you have the R package [`dplyr`](https://dplyr.tidyverse.org/) installed. \n",
    "\n",
    "Our example function for testing is taken from the **Writing functions in R** short course. The function below is designed to take in medical data of subjects that contains a column `BMI` of body mass index measurements. After filtering the data to remove rows that have missing BMI scores, it adds a column of discrete BMI categories, based on the following ranges:\n",
    "\n",
    "* Normal: 18.5 <= BMI < 25\n",
    "* Overweight: 25 <= BMI < 30\n",
    "* Obese: 30 <= BMI\n",
    "* Underweight: BMI < 18.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253555e6-3161-4259-b7d8-af271db0fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Add a column of BMI categories\n",
    "#'\n",
    "#' The following ranges are used for the categorization:\n",
    "#'\n",
    "#' Normal:      18.5 <= BMI < 25\n",
    "#' Overweight:  25 <= BMI < 30\n",
    "#' Obese:       30 <= BMI\n",
    "#' Underweight: BMI < 18.5\n",
    "#'\n",
    "#' If `data` has rows that are missing a BMI score then these rows will be\n",
    "#' dropped and a warning message displayed.\n",
    "#'\n",
    "#' @param data A dataframe containing a column 'BMI' of numerical BMI scores.\n",
    "#'\n",
    "#' @returns A dataframe with a new factor column 'BMI_cat', with the levels\n",
    "#'   ordered \"Normal\", \"Overweight\", \"Obese\", \"Underweight\".\n",
    "add_BMI_categories <- function(data) {\n",
    "  BMI_data <- data |>\n",
    "    dplyr::filter(!is.na(BMI))\n",
    "  \n",
    "  if (nrow(BMI_data) < nrow(data)) {\n",
    "    n_dropped_rows = nrow(data) - nrow(BMI_data)\n",
    "    warning(\"Dropped \", n_dropped_rows, \" rows with missing BMI readings\")\n",
    "  }\n",
    "  \n",
    "  BMI_levels <- c(\"Normal\", \"Overweight\", \"Obese\", \"Underweight\")\n",
    "  BMI_data <- BMI_data |>\n",
    "    dplyr::mutate(\n",
    "      BMI_cat = dplyr::case_when(\n",
    "        BMI >= 18.5 & BMI < 25 ~ \"Normal\",\n",
    "        BMI >= 25 & BMI < 30 ~ \"Overweight\",\n",
    "        BMI >= 30 ~ \"Obese\",\n",
    "        BMI < 18.5 ~ \"Underweight\"\n",
    "      )\n",
    "    ) |>\n",
    "    dplyr::mutate(BMI_cat = factor(BMI_cat, levels = BMI_levels))\n",
    "  \n",
    "  return(BMI_data)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a366a-1278-4dc1-89d7-3bc43b60a894",
   "metadata": {},
   "source": [
    "### Exercise: What to test?\n",
    "\n",
    "Before continuing, look at the description of the function above and write down all the tests that you think should be written for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4759c5-2bcf-4e88-b249-be30c172c2b6",
   "metadata": {},
   "source": [
    "### Starting our test suite\n",
    "\n",
    "We think the following would be good things to test for `add_BMI_categories` (perhaps you thought of others!):\n",
    "* Categorises BMI within the above ranges correctly (the normal case, a.k.a. 'happy path').\n",
    "* Categorises BMI values at the boundary points of the above ranges correctly (edge cases).\n",
    "* Filters out any rows with missing `BMI` values.\n",
    "* Gives a warning with the correct number of dropped rows, if there were missing `BMI` values.\n",
    "* Leaves the original columns in the dataframe untouched (both in the case where there are missing `BMI` values and where there are not).\n",
    "\n",
    "We could also test the following, though in that case we'd be tempted to modify our function give better error messages than the ones automatically produced by R:\n",
    "* Gives an error if the `BMI` column is missing.\n",
    "* Gives an error if the `BMI` column is not numeric.\n",
    "\n",
    "Let's write a test for the first of the above cases: the 'happy path'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ea28ec-5b61-4716-8ff6-6d52423cda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTest passed\u001b[39m ğŸŒˆ\n"
     ]
    }
   ],
   "source": [
    "test_that(\"add_BMI_categories categorises BMI within the correct ranges\", {\n",
    "  data <- data.frame(BMI = c(18, 30.1, 27.5, 20))\n",
    "  \n",
    "  BMI_levels <- c(\"Normal\", \"Overweight\", \"Obese\", \"Underweight\")\n",
    "  expected_BMI_cats <- factor(\n",
    "    c(\"Underweight\", \"Obese\", \"Overweight\", \"Normal\"),\n",
    "    levels = BMI_levels\n",
    "  )\n",
    "  expected_result <- data |>\n",
    "    dplyr::mutate(BMI_cat = expected_BMI_cats)\n",
    "  \n",
    "  expect_equal(add_BMI_categories(data), expected_result)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be26946-4c43-4054-ba6f-32e7cb4dffbb",
   "metadata": {},
   "source": [
    "Observe the following:\n",
    "- We can put multiple lines of code within a test. In fact, it's rare to be able to write tests in just one line.\n",
    "- We cooked up a simple dataframe `data` that allowed us to test the behaviour we wanted. In this case, we know what the correct answer should be, so can test the output of `add_BMI_categories` directly. In general, we want our test data to give a fair representation of what we're trying to test, but beyond that it should be simple e.g. don't use 10 rows when 4 will suffice.\n",
    "- We've tried to keep our test code as easy to read as possible and adopt good programming practices. This is important, because we need to check that the test is correct by eye!\n",
    "- The overall structure of the test can be broken down into three parts, sometimes referred to as 'Arrange-Act-Assert':\n",
    "  1. Set up the arguments we want to supply to our function-under-test.\n",
    "  2. Call the function.\n",
    "  3. Check something about the result. (In the above code, 2 and 3 are merged into the last line.)\n",
    "\n",
    "Other than that, there's really nothing more to writing tests as for writing any other code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eeb085-a288-410b-acd0-5f41a46111ba",
   "metadata": {},
   "source": [
    "### Exercise: Continuing the test suite\n",
    "\n",
    "Implement the other tests that were identified above (and/or implement the tests you came up with).\n",
    "\n",
    "### Exercise: Changing the function's behaviour\n",
    "\n",
    "Suppose we no longer want to filter out the rows with missing `BMI` values, but instead should add `NA` as the `BMI_cat` for any rows where `BMI` is missing.\n",
    "1. Remove any tests that tested the old filtering behaviour.\n",
    "2. Modify the code of the function to do this and write test(s) to check this new behaviour. If you want to experiment with something new, try doing this by implementing the test(s) **before** changing the function, in an incremental fashion: add a test, then change the function just enough to make it pass the new test (and also the old tests), then repeat this process with any extra tests until you've made all the required changes to the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011fbd4-b41c-4285-a876-a3f8e7b6011c",
   "metadata": {},
   "source": [
    "## General advice / tips\n",
    "\n",
    "We conclude this tutorial on unit testing with some general advice and references to further information for more advanced testing cases. An excellent general guide to using `testthat` and how to think about testing in R can be found in the [R Packages book](https://r-pkgs.org/) (chapters 13 â€“ 15 at the time of writing).\n",
    "\n",
    "\n",
    "### The testing mindset\n",
    "\n",
    "When we test a function, we are basically putting it under scrutiny: we try to think about ways it could go wrong and check that it behaves correctly. Often this includes considering cases we might not have thought about when we wrote the original function: for example, what if `NULL` or `NA` gets supplied as the value of one of the arguments? Or what if a function that takes in a vector receives a vector of length zero? Or what if there are no rows left after doing some filtering of a dataframe?\n",
    "\n",
    "\n",
    "### Keep the tests passing!\n",
    "\n",
    "Hopefully this one is obvious: Always ensure that **all** your tests pass. If a test starts to fail, then investigate why â€” don't just ignore it! If it shows that there is a bug in your code, then fix it. If on the other hand the test needs updating to be correct, then change it. Or, if the test is no longer relevant, then delete it. \n",
    "\n",
    "\n",
    "### Re-run all the tests whenever you change your code\n",
    "\n",
    "The whole point of having a suite of unit tests is to catch bugs that might have unexpectedly crept into our code after we make a change. Therefore, run your tests whenever you make changes to your functions that are under test. We want to stress two points here:\n",
    "- You should run **all** of your tests, not just the ones for the function(s) you have changed. This is to ensure that your changes haven't adversely affected any other functions that rely on the modified functions. In practice, using `testthat::test_dir` means it's no harder to run all your tests than just a few of them.\n",
    "- Don't make a big change to your functions all in one go without running your tests. Instead, break down big changes into smaller steps and run the tests after each of the smaller steps. If at any point a test fails after a change, having made smaller changes will help you identify the cause of the failure much more quickly than a whole lot of changes. For the same reason, where possible we recommended only changing one function at a time between running tests.\n",
    "\n",
    "\n",
    "### Keep tests organised\n",
    "\n",
    "We've already discussed how the usual convention is to have one test file for each R source file. In addition to this, keep all the tests within a test file grouped together by function, as this makes it easier to navigate and find your tests. Also make it clear in the description argument of `test_that` which function is being tested, for example:\n",
    "\n",
    "```r\n",
    "test_that(\"my_func does such-and-such a thing\", {...})\n",
    "```\n",
    "\n",
    "### Isolated tests\n",
    "\n",
    "Strive to ensure that each `test_that` test can be run completely on its own and that all the tests in your test files can be run in any order: don't assume that `testthat` will run the tests in a particular order. In particular, modifying global variables that are shared between tests is a very bad idea, because executing one test affects the setup conditions for the other tests in this case (which is a very tricky source of bugs). It's OK to repeat code / data between tests, because this keeps them independent. If you really need to use the same object between tests, look into [using test fixtures](https://r-pkgs.org/testing-advanced.html#test-fixtures) instead.\n",
    "\n",
    "\n",
    "### Test behaviour, not implementation\n",
    "\n",
    "In a nutshell, functions are like little machines that take input (through the arguments) and then return some value as the output and/or have some other side effect (like printing a message to the console or writing a file). Focus on testing the **behaviour** of functions and not how they're implemented. Put another way, how could you test that this machine is working correctly without peering into its internal mechanics? This approach is often called **black-box testing**: we test the function based just on its 'interface' (the inputs, outputs and any side effects). The advantage of this approach is that it allows us to change the internal workings of the function without affecting the tests. Another way to think about this: write your tests as if they were little snippets of example code showing a user of the function what it does and how to use it.\n",
    "\n",
    "\n",
    "### How much testing?\n",
    "\n",
    "Once you get into testing, it can sometimes be difficult to know where to stop and when you've done 'enough'. It's difficult to give a hard-and-fast rule about this because it's context specific. But you might find it helpful to think in terms of taking a risk-based approach. In general, the risk of an event is defined as the multiple of (1) the impact of of the event occuring, and (2) the likelihood of it occuring. So consider focussing on cases where the consequences of incorrect behaviour are significant (such as an incorrect calculation silently carrying through the analysis) or which are likely to occur (e.g. testing a function that cleans data that is very likely to contain missing values). Pay particular attention to the cases which are high risk i.e. likely to occur **and** that have a significantly negative impact if the code is wrong! On the other hand, if something is unlikely to happen and the consequences of incorrect code are minor then maybe it's not worth worrying about testing it.\n",
    "\n",
    "\n",
    "### Keep tests simple\n",
    "\n",
    "Strive to keep tests simple. Sometimes we find that to test a particular behaviour of a function, we need to do quite a lot of set-up code or it's difficult to 'detect' the thing we're trying to check. In that case, consider whether you could break up the function into smaller parts that can be tested independently, or whether maybe that function is not quite the 'right' function and rewriting it would make it easier to test. Often, allowing the tests to inform the design of your functions in this way leads to code which is easier to change and understand, because it is broken down into pieces that can be changed / replaced with minimal impact on other parts of the code. Aside from this, keeping tests simple means we are less likely to make mistakes when writing the tests themselves.\n",
    "\n",
    "\n",
    "### Bug --> test\n",
    "\n",
    "Whenever you discover a bug in your code, before fixing your code write a test that detects that bug (i.e. fails initially but will pass when the code is corrected). This is a great way of building up a suite of tests and ensuring that bugs don't re-emerge in the future.\n",
    "\n",
    "\n",
    "### Tools for testing more challenging things\n",
    "\n",
    "Occasionally you might need to write a test that involves making a change outside the test itself, like writing a file or updating some global setting. In this case, we need to ensure that the state of the system is returned to how it was before the test was run â€” not doing so can lead to situations where tests start incorrectly failing for unapparent reasons. The `testthat` package provides **test fixtures** for situations like these: see this [vignette on test fixtures](https://testthat.r-lib.org/articles/test-fixtures.html) for more details.\n",
    "\n",
    "We'll also mention here that the `testthat` package provides something called **snapshots** to help writing tests where the expected output is difficult to create manually, where we mostly just want to check that the output is 'the same' each time we run the test. This can be particularly helpful e.g. for checking that an image file of a graph hasn't changed. See this [vignette on snapshot tests](https://testthat.r-lib.org/articles/snapshotting.html) for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
