{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Python Checking and Testing Code\n",
    "\n",
    "\n",
    "```{card}\n",
    "\n",
    "<div class=\"alert alert-block\" style=\"background-color: #ffffff; border: 1px solid #ccc; padding: 1em; border-radius: 8px;\">\n",
    "\n",
    "**Author:** Michael Saunby (GitHub: [msaunby](https://github.com/msaunby))\n",
    "\n",
    "**License:** Creative Commons Attribution-ShareAlike 4.0 International license ([CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)).\n",
    "\n",
    "</div>\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Course Objectives\n",
    "- Understand the importance and limitations of software testing\n",
    "- Recognize different types of testing such as system-level and defect testing\n",
    "- Learn how to implement and interpret assert statement in Python code\n",
    "- Understand when and why to use assert statements\n",
    "- Understand the concept of unit testing and Test Driven Development (TDD)\n",
    "- Write and execute unit tests using the unit test module\n",
    "- Learn the purpose and implementation of fixtures and mocks in testing\n",
    "- Apply these concepts to test code that depends on external resources\n",
    "- Understand the role of code linting and type checking\n",
    "- Use tools like Flake8 and my[py] to ensure code quality and adherence to style guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Testing software\n",
    "\n",
    "Although we cannot prove our code is free of defects or bugs, we can, and should, establish that it behaves as intended.\n",
    "\n",
    "\n",
    "\n",
    "### System level testing\n",
    "\n",
    "Once we have completed our software, we should ensure that it works as intended.  This is referred to as validation testing. Typically, this will involve taking a sample of input data and ensuring that the output of our software is as expected for the given input.\n",
    "\n",
    "This *validation* testing will tell us if the overall system runs and produces valid results.  Such tests should be repeated when changes are made to the software to ensure the changes have not introduced errors.\n",
    "\n",
    "Changes that can impact your software are diverse and include:\n",
    "\n",
    " * new Python language releases\n",
    " \n",
    " * upgrades to imported libraries\n",
    " \n",
    " * operating system updates.\n",
    "\n",
    "### Defect testing\n",
    "\n",
    "In a research environment it is often the case that there is no explicit specification for the software we create.\n",
    "\n",
    "By specification we mean something like:\n",
    "\n",
    "* Written statement of user requirements - typically \"user stories\"\n",
    "\n",
    "* Functional requirements - e.g what file formats are to be supported\n",
    "\n",
    "* Non-functional requirements - e.g. subject data must be encrypted.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "If you don't have a specification for your software, how might you establish suitable tests to find and resolve defects?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Assert statement\n",
    "\n",
    "The built-in Python assert statement looks like this -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try modifying this code to deliberately fail the assert statements\n",
    "\n",
    "def my_add_two(a):\n",
    "    return a + 2.0\n",
    "\n",
    "assert my_add_two(1) == 3\n",
    "# Better to include a message in case of failure\n",
    "assert my_add_two(3) == 5, f\"my_add_two(3) failed with {my_add_two(3)}, expected 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## When to use assert\n",
    "\n",
    "```assert``` should never be used to modify control flow.\n",
    "\n",
    "Assertions allow you to verify that parts of your program are correct, but are only applied if the internal constant ```__debug__``` is ```True```.  Although ```__debug__``` is usually set to True, it is not guaranteed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is approximate what the assert statement does \n",
    "\n",
    "def my_assert(condition, message):\n",
    "    if __debug__ and not condition:\n",
    "        raise AssertionError(message)\n",
    "\n",
    "my_assert(my_add_two(1) == 3, \"my_add_two(1) failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Why might we want different behaviour from our assert statements?\n",
    "\n",
    "### What would you want your assert statements to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Unit-tests\n",
    "\n",
    "Unit-tests are small tests that test the behaviours of our functions and classes.\n",
    "\n",
    "Unit-tests are typically run within a testing framework or test-runner that automates testing, often inside our IDE.\n",
    "\n",
    "### Test Driven Development (TDD)\n",
    "\n",
    "TDD is an approach to software design, it is not software testing. TDD uses unit-tests to create a software design, especially when the design is created incrementally, as with Agile.\n",
    "\n",
    "### Refactoring\n",
    "\n",
    "Whether or not you adopt TDD, refactoring - changing the implementation of your code without changing its behaviour, is something that you are certain to do. If only to remove print statements, or change the names of variables.\n",
    "\n",
    "Refactoring code without appropriate tests can easily introduce new errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## unittest\n",
    "\n",
    "Python 3 distributions include the unittest module.  See https://docs.python.org/3/library/unittest.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMyAddTwo(unittest.TestCase):\n",
    "    def test_my_add_two(self):\n",
    "        self.assertEqual(my_add_two(1), 3)\n",
    "    def test_my_add_two_3(self):\n",
    "        self.assertEqual(my_add_two(3), 5)\n",
    "\n",
    "# unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Fixtures and mocks\n",
    "\n",
    "Ideally each unit of code should be tested independently.\n",
    "\n",
    "### Why is this?\n",
    "\n",
    "However, there are situations where testing code might require data read from a file or a database connection. If only one test requires this external data, then opening the file and reading the data will be part of the test. If several tests require this data, then we use a fixture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module level fixture setup and teardown\n",
    "def setUpModule():\n",
    "    global sample_data\n",
    "    sample_data = open(\"data/rows.txt\", \"r\")\n",
    "\n",
    "def tearDownModule():\n",
    "    sample_data.close()\n",
    "\n",
    "# unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class level fixture setup and teardown\n",
    "class TestMyAddTwo(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.sample_data = open(\"data/rows.txt\", \"r\")\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        cls.sample_data.close()\n",
    "\n",
    "    def test_file_parsing(self):\n",
    "        for line in self.sample_data:\n",
    "            # Do something with the line\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Mocks\n",
    "\n",
    "Mock and MagicMock objects create all attributes and methods as you access them and store details of how they have been used. You can configure them to specify return values or limit what attributes are available.\n",
    "\n",
    "See https://docs.python.org/3/library/unittest.mock.html\n",
    "\n",
    "\n",
    "### How can we test a function that does not return a value?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def show_results():\n",
    "    arr = [1, 2, 3]\n",
    "    print(arr)\n",
    "    print()\n",
    "\n",
    "show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Here is a possible test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock\n",
    "\n",
    "class TestShowResults(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        global print\n",
    "        print = MagicMock()\n",
    "    def tearDown(self):\n",
    "        global print\n",
    "        print = __builtins__.print\n",
    "    def test_show_results(self):\n",
    "        show_results()\n",
    "        self.assertEqual(print.call_count, 2)\n",
    "\n",
    "# unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## How does this test work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 'Linting' code with Flake8 and my[py]\n",
    "\n",
    "There are various tools that can analyse Python code and suggest fixes or improvements without running the code.\n",
    "\n",
    "These are 'static code checkers' or 'linters' - because they help you remove fluff!\n",
    "\n",
    "### my[py]\n",
    "We saw my[py] briefly before. It is used to find mistakes in type hints, and can even be used to enforce type hints if desired.\n",
    "\n",
    "https://mypy.readthedocs.io/en/stable/getting_started.html\n",
    "\n",
    "### Flake8\n",
    "Flake8 runs a variety of checks on your Python scripts, and can be used with IDEs such as VS Code to help you write clearer, more readable, code.  The, optional, but highly recommended style guide for Python is PEP 8.\n",
    "\n",
    "https://peps.python.org/pep-0008/\n",
    "\n",
    "https://flake8.pycqa.org/en/latest/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Test coverage\n",
    "\n",
    "### Coverage.py works in three phases:\n",
    "\n",
    "* Execution: Coverage.py runs your code, and monitors it to see what lines were executed.\n",
    "\n",
    "* Analysis: Coverage.py examines your code to determine what lines could have run.\n",
    "\n",
    "* Reporting: Coverage.py combines the results of execution and analysis to produce a coverage number and an indication of missing execution.\n",
    "\n",
    "See https://coverage.readthedocs.io/en/7.5.3/api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Pytest\n",
    "\n",
    "### Introduction to pytest\n",
    "\n",
    "Pytest is another popular tool for testing in Python which makes it easier to write and run tests. \n",
    "\n",
    "Pytest uses file and function naming conventions to discover test.  You will rarely need to run a test directly as the framework will find and run tests for you when you modify your code.\n",
    "\n",
    "Pytest is a package that you install into your environment from conda or PyPI. For example:\n",
    "\n",
    "```sh\n",
    "pip install pytest\n",
    "```\n",
    "\n",
    "You should then create a directory containing your tests called `tests`.\n",
    "\n",
    "```sh\n",
    "mkdir tests\n",
    "```\n",
    "\n",
    "Within tests, you can create Python scripts containing tests - e.g. `example_test.py`.\n",
    "\n",
    "### Example test: comparing csv files\n",
    "\n",
    "There are a wide variety of applications for testing. Below is an example of a test where we are running some code to generate a `.csv` file, and then confirming if the results are as expected.\n",
    "\n",
    "This could come in handy if you have produced code for a model, and are concerned that others running on the model on a different machine could be getting slightly different results. \n",
    "\n",
    "In this example, we start the `.py` file by importing:\n",
    "\n",
    "* `pytest` (to run the tests)\n",
    "* `pandas` (to manage the csv files)\n",
    "* Our model (imagining a scenario where we have a file `model_code` inside a folder `scripts/`, which is a sister folder to `tests/`)\n",
    "* `tempfile` (to save our model results to a temporary directory)\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from scripts import model_code\n",
    "import tempfile\n",
    "```\n",
    "\n",
    "We then provide file paths at the start of our script:\n",
    "\n",
    "```python\n",
    "EXP_FOLDER = 'exp_results'\n",
    "TEMP_FOLDER = tempfile.mkdtemp()\n",
    "```\n",
    "\n",
    "Assuming we might be running the model with two different parameters (so producing two `.csv` files, and comparing each of those), we can use `parametrise` to run the same test with two different inputs. First, we can define our parameters for the model:\n",
    "\n",
    "```python\n",
    "parameters = [\n",
    "    {\n",
    "        'arrivals': 100,\n",
    "        'file': 'result100.csv'\n",
    "    },\n",
    "    {\n",
    "        'arrivals': 150,\n",
    "        'file': 'result150.csv'\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "For the file paths, we should set these up as fixtures:\n",
    "\n",
    "```python\n",
    "@pytest.fixture\n",
    "def exp_folder():\n",
    "    return EXP_FOLDER\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def temp_folder():\n",
    "    return TEMP_FOLDER\n",
    "```\n",
    "\n",
    "We can then write our test function, and use the file names from parameters as the ID for each test:\n",
    "\n",
    "```python\n",
    "@pytest.mark.parametrize('param', parameters,\n",
    "                         ids=[d['file'] for d in parameters])\n",
    "def test_equal_df(param, temp_folder, exp_folder):\n",
    "    '''\n",
    "    Test that model results are consistent with the expected\n",
    "    results (which are saved in the EXP_FOLDER)\n",
    "    '''\n",
    "    # Run the model (assuming the function has inputs for our\n",
    "    # parameter dictionary and for a save location for the .csv file)\n",
    "    model_code.main(**param, temp_folder)\n",
    "\n",
    "    # Import the test and expected results (we can use the filename from the\n",
    "    # parameter dictionary, and then the folder name)\n",
    "    test_result = import_xls(temp_folder, param['file'])\n",
    "    exp_result = import_xls(exp_folder, param['file'])\n",
    "\n",
    "    # Check that the dataframes are equal\n",
    "    pd.testing.assert_frame_equal(test_result, exp_result)\n",
    "```\n",
    "\n",
    "With our `.py` file now complete, we can run our tests from the terminal. Ensuring you are located in the parent folder to `tests/`, run the command:\n",
    "\n",
    "```sh\n",
    "pytest\n",
    "```\n",
    "\n",
    "If your tests take a long time to run, you may want to explore parallelising them. You can install `pytest-xdist`, which is a package that parallelises your pytests using multiple CPUs. With this package installed, you can run the command:\n",
    "\n",
    "```sh\n",
    "pytest -n auto\n",
    "```\n",
    "\n",
    "### Coverage\n",
    "\n",
    "https://pypi.org/project/pytest-cov/\n",
    "\n",
    "### pytest-notebook\n",
    "\n",
    "See https://pytest-notebook.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Resources\n",
    "\n",
    "See the testing section of https://alan-turing-institute.github.io/rse-course/html/module01_introduction_to_python/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Testing Practical Exercise\n",
    "\n",
    "Python 3 distributions include the unittest module.  See https://docs.python.org/3/library/unittest.html\n",
    "\n",
    "Here is the example included in the Python documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Exercise 1\n",
    "\n",
    "Using the above as a template, create a test class for the Upper class we used earlier.\n",
    "\n",
    "```python\n",
    "class Upper(str):\n",
    "    def __new__(cls, text=\"\"):\n",
    "        return super().__new__(cls, text.upper())\n",
    "```\n",
    "\n",
    "### Important\n",
    "\n",
    "What should (and can) be tested?\n",
    "\n",
    "See https://docs.python.org/3/library/unittest.html\n",
    "\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "Design a new capability for the class using TDD.\n",
    "\n",
    "Here are some suggestions -\n",
    "\n",
    "* Do not allow strings without at least one letter\n",
    "\n",
    "* Only allow strings that begin with a letter\n",
    "\n",
    "* Limit the length of the string to 10 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
