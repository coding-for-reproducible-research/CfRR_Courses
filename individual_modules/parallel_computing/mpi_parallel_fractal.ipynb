{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e355674-9902-4e67-8517-1706d50b9f23",
   "metadata": {},
   "source": [
    "# MPI real world example\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, learners will be able to:\n",
    "\n",
    "- Parallelize the fractal generation problem using MPI to improve performance and distribute the workload across multiple processes.\n",
    "- Download and run both the serial and parallel versions of the Julia set fractal code.\n",
    "- Observe performance improvements when switching from serial to parallel execution with MPI.\n",
    "  \n",
    "## MPI real world example problem\n",
    "\n",
    "In a previous lesson we have seen *multi-processing* being used to solve the generation of the Julia set. An alternative approach is to use *message passing*.\n",
    "\n",
    "As mentioned earlier, this is a relatively simple problem to parallelise. If we consider running the program with multiple processes, all we need to do to divide the work is to divide the complex grid up between the processes. Thinking back to previous sections, we covered an MPI function that can achieve this - the `scatter` method of the MPI communicator.\n",
    "\n",
    "We can directly take the example from the previous chapter and apply it to the complex mesh creation function:\n",
    "\n",
    "```python\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "if comm.Get_rank() == 0:\n",
    "    grid = complex_grid(extent, cells)\n",
    "    grid = np.array_split(grid, comm.Get_size())\n",
    "else:\n",
    "    grid = None\n",
    "\n",
    "grid = comm.scatter(grid, root=0)\n",
    "```\n",
    "\n",
    "Here we are following the same pattern of initialising data on the root rank, splitting into equal-ish parts and scattering to all the different ranks. Each rank can then apply the Julia set function to it's own part of the mesh - this part of the code doesn't need to change at all!\n",
    "To complete the process, we need to gather the data back into a single array. We can do this with the communicator's `gather` method, followed by concatenating the resulting array:\n",
    "\n",
    "```python\n",
    "fractal = comm.gather(fractal, root=0)\n",
    "if not fractal is None:\n",
    "    fractal = np.concatenate(fractal)\n",
    "```\n",
    "\n",
    "With this method we have effectively offloaded the work of the function to multiple processes and ended up with the same result. Let's use `time` to see if we have increased the speed of the function:\n",
    "\n",
    "```\n",
    "$ time python parallel_fractal.py\n",
    "python parallel_fractal.py  21.52s user 14.17s system 93% cpu 38.368 total\n",
    "\n",
    "$ time mpirun -n 4 python parallel_fractal.py\n",
    "mpirun -n 4 python parallel_fractal.py  37.23s user 21.70s system 370% cpu 15.895 total\n",
    "```\n",
    "\n",
    "We can see that running the problem in parallel has greatly increased the speed of the function, but that the speed increase is directly proportional to the resource we are using (i.e. using 4 cores doesn't make the process 4 times faster). This is due to the increased overhead induced by MPI communication procedures, which can be quite expensive (as mentioned in previous chapters).\n",
    "The way that a program performance changes based on the number of processes it runs on is often referred to as its \"scaling behaviour\". Determining how your problem scales across multiple processes is a useful exercise and is helpful when it comes to porting your code to a larger scale HPC machine.\n",
    "\n",
    "### Download Complete Parallel File \n",
    "[Download complete parallel_fractal_example file](complete_files/mpi_fractal_complete.py)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
