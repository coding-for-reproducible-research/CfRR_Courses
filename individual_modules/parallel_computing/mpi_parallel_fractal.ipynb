{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e355674-9902-4e67-8517-1706d50b9f23",
   "metadata": {},
   "source": [
    "# MPI real world example\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, learners will be able to:\n",
    "\n",
    "- Parallelize the fractal generation problem using MPI to improve performance and distribute the workload across multiple processes.\n",
    "- Download and run both the serial and parallel versions of the Julia set fractal code.\n",
    "- Observe performance improvements when switching from serial to parallel execution with MPI.\n",
    "  \n",
    "## MPI real world example problem\n",
    "\n",
    "The problem we will attempt to solve is constructing a fractal. This kind of problem is often known as \"embarrassingly parallel\" meaning that each element of the result has no dependency on any of the other elements, meaning that we can solve this problem in parallel without too much difficulty. \\Let's get started by creating a new script - `mpi_fractal.py`:\n",
    "\n",
    "### Setting up our serial problem\n",
    "\n",
    "Let's first think about our problem in serial - we want to construct the [Julia set](https://en.wikipedia.org/wiki/Julia_set) fractal, so we need to create a grid of complex numbers to operate over. We can create a simple function to do this:\n",
    "\n",
    "```python\n",
    "# fractal.py\n",
    "import numpy as np\n",
    "\n",
    "def complex_grid(extent, n_cells, grid_range):\n",
    "    mesh_range = np.arange(-extent, extent, extent/ncells)\n",
    "    x, y = np.meshgrid(grid_range * 1j, grid_range)\n",
    "    z = x + y\n",
    "\n",
    "    return z\n",
    "```\n",
    "\n",
    "Now, we can create a function that will calculate the Julia set convergence for each element in the complex grid:\n",
    "\n",
    "```python\n",
    "import warnings\n",
    "\n",
    "...\n",
    "\n",
    "def julia_set(grid):\n",
    "\n",
    "    fractal = np.zeros(np.shape(grid))\n",
    "\n",
    "    # Iterate through the operation z := z**2 + c.\n",
    "    for j in range(num_iter):\n",
    "        grid = grid ** 2 + c\n",
    "        # Catch the overflow warning because it's annoying\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            index = np.abs(grid) < np.inf\n",
    "        fractal[index] = fractal[index] + 1\n",
    "\n",
    "    return fractal\n",
    "```\n",
    "\n",
    "This function calculates how many iterations it takes for each element in the complex grid to reach infinity (if ever) when operated on with the equation `x = x**2 + c`. The function itself is not the focus of this exercise as much as it is a way to make the computer perform some work! Let's use these functions to set up our problem in serial, without any parallelism:\n",
    "\n",
    "```python\n",
    "\n",
    "...\n",
    "\n",
    "c = -0.8 - 0.22 * 1j\n",
    "extent = 2\n",
    "cells = 2000\n",
    "\n",
    "grid = complex_grid(extent, cells)\n",
    "fractal = julia_set(grid, 80, c)\n",
    "```\n",
    "\n",
    "If we run the python script (`python fractal.py`) it takes a few seconds to complete (this will vary depending on your machine), so we can already see that we are making our computer work reasonably hard with just a few lines of code. If we use the `time` command we can get a simple overview of how much time and resource are being used:\n",
    "\n",
    "```\n",
    "$ time python parallel_fractal_complete.py\n",
    "python parallel_fractal_complete.py  5.96s user 3.37s system 123% cpu 7.558 total\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{note}\n",
    " We can also visualise the Julia set with the code snippet:\n",
    "`\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "...\n",
    "\n",
    "plt.imshow(fractal, extent=[-extent, extent, -extent, extent], aspect='equal')\n",
    "plt.show()\n",
    "`\n",
    "but doing so will impact the numbers returned when we time our function, so it's important to remember this before trying to measure how long the function takes.\n",
    "```\n",
    "\n",
    "### Download Complete Serial File \n",
    "[Download complete serial fractal example file](complete_files/fractal_complete.py)\n",
    "\n",
    "### Parallelising our serial problem\n",
    "\n",
    "Next we are going to sovle the Julia set problem in parallel using *message passing*. As mentioned earlier, this is a relatively simple problem to parallelise. If we consider running the program with multiple processes, all we need to do to divide the work is to divide the complex grid up between the processes. Thinking back to previous sections, we covered an MPI function that can achieve this - the `scatter` method of the MPI communicator.\n",
    "\n",
    "We can directly take the example from the previous chapter and apply it to the complex mesh creation function:\n",
    "\n",
    "```python\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "if comm.Get_rank() == 0:\n",
    "    grid = complex_grid(extent, cells)\n",
    "    grid = np.array_split(grid, comm.Get_size())\n",
    "else:\n",
    "    grid = None\n",
    "\n",
    "grid = comm.scatter(grid, root=0)\n",
    "```\n",
    "\n",
    "Here we are following the same pattern of initialising data on the root rank, splitting into equal-ish parts and scattering to all the different ranks. Each rank can then apply the Julia set function to it's own part of the mesh - this part of the code doesn't need to change at all!\n",
    "To complete the process, we need to gather the data back into a single array. We can do this with the communicator's `gather` method, followed by concatenating the resulting array:\n",
    "\n",
    "```python\n",
    "fractal = comm.gather(fractal, root=0)\n",
    "if not fractal is None:\n",
    "    fractal = np.concatenate(fractal)\n",
    "```\n",
    "\n",
    "With this method we have effectively offloaded the work of the function to multiple processes and ended up with the same result. Let's use `time` to see if we have increased the speed of the function:\n",
    "\n",
    "```\n",
    "$ time python parallel_fractal.py\n",
    "python parallel_fractal.py  21.52s user 14.17s system 93% cpu 38.368 total\n",
    "\n",
    "$ time mpirun -n 4 python parallel_fractal.py\n",
    "mpirun -n 4 python parallel_fractal.py  37.23s user 21.70s system 370% cpu 15.895 total\n",
    "```\n",
    "\n",
    "We can see that running the problem in parallel has greatly increased the speed of the function, but that the speed increase is directly proportional to the resource we are using (i.e. using 4 cores doesn't make the process 4 times faster). This is due to the increased overhead induced by MPI communication procedures, which can be quite expensive (as mentioned in previous chapters).\n",
    "The way that a program performance changes based on the number of processes it runs on is often referred to as its \"scaling behaviour\". Determining how your problem scales across multiple processes is a useful exercise and is helpful when it comes to porting your code to a larger scale HPC machine.\n",
    "\n",
    "### Download Complete Parallel File \n",
    "[Download complete parallel_fractal_example file](complete_files/mpi_fractal_complete.py)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
